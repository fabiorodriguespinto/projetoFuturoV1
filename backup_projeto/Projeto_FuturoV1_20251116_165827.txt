==============================
ğŸ“ Estrutura de diretÃ³rios :: /opt/Projetos/Projeto_FuturoV1
Gerado: 2025-11-16 16:58:27-03:00
==============================

.
â”œâ”€â”€ api
â”‚Â Â  â”œâ”€â”€ app
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ models.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ routers
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ .gitignore
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ healthcheck.py
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ predict.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ services
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ database.py
â”‚Â Â  â”‚Â Â      â””â”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â””â”€â”€ requirements.txt
â”œâ”€â”€ config
â”‚Â Â  â””â”€â”€ config.yaml
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ app.db
â”‚Â Â  â”œâ”€â”€ input
â”‚Â Â  â”œâ”€â”€ modelo
â”‚Â Â  â”œâ”€â”€ output
â”‚Â Â  â””â”€â”€ trades.db
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ grafana
â”œâ”€â”€ init
â”‚Â Â  â”œâ”€â”€ db_init.py
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â””â”€â”€ requirements.txt
â”œâ”€â”€ neural_network
â”‚Â Â  â”œâ”€â”€ inference
â”‚Â Â  â”‚Â Â  â””â”€â”€ predict.py
â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ models.py
â”‚Â Â  â”œâ”€â”€ requirements_neural_network.txt
â”‚Â Â  â””â”€â”€ training
â”‚Â Â      â”œâ”€â”€ __init__.py
â”‚Â Â      â”œâ”€â”€ train_bkp2.py
â”‚Â Â      â”œâ”€â”€ train_bkp3.py
â”‚Â Â      â”œâ”€â”€ train_linear.py
â”‚Â Â      â”œâ”€â”€ train.py
â”‚Â Â      â”œâ”€â”€ train_rf.py
â”‚Â Â      â”œâ”€â”€ train_xgb.py
â”‚Â Â      â”œâ”€â”€ treinar_linear.py
â”‚Â Â      â”œâ”€â”€ treinar_modelo.py
â”‚Â Â      â””â”€â”€ utils.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ bin
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ iniciar.sh
â”‚Â Â  â”‚Â Â  â””â”€â”€ start_all.sh
â”‚Â Â  â””â”€â”€ coleta
â”‚Â Â      â””â”€â”€ coleta_cripto.py
â”œâ”€â”€ shared
â”‚Â Â  â”œâ”€â”€ config.py
â”‚Â Â  â”œâ”€â”€ database.py
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ market_data.py
â”‚Â Â  â”œâ”€â”€ models.py
â”‚Â Â  â””â”€â”€ telegram_bot.py
â”œâ”€â”€ test_telegram.py
â””â”€â”€ worker
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ main.py
    â”œâ”€â”€ market_collector.py
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ tasks
        â”œâ”€â”€ __init__.py
        â””â”€â”€ retrain_model.py

23 directories, 52 files

==============================
ğŸ“„ ConteÃºdo dos arquivos
==============================

----------------------------------------
Arquivo: ./api/app/routers/__init__.py
----------------------------------------
# Router init

----------------------------------------
Arquivo: ./api/app/routers/predict.py
----------------------------------------
from fastapi import APIRouter
from shared.database import carregar_candles
from shared import config
import joblib
import numpy as np
import pandas as pd
import os

router = APIRouter(prefix="/predict", tags=["PrediÃ§Ã£o"])

MODEL_DIR = "/app/neural_network/models/"


@router.post("/")
def prever():
    df = carregar_candles(limit=config.NUM_CANDLES_USADOS)

    if df is None or len(df) < config.NUM_CANDLES_USADOS:
        return {"erro": "Dados insuficientes no banco."}

    X = df.tail(config.NUM_CANDLES_USADOS)["close"].pct_change().fillna(0).values
    X = np.array([X])

    modelos = {}
    previsoes = []

    for nome_arquivo in os.listdir(MODEL_DIR):
        if nome_arquivo.endswith(".pkl"):
            modelo = joblib.load(os.path.join(MODEL_DIR, nome_arquivo))
            y = modelo.predict(X)[0]
            modelos[nome_arquivo] = float(y)
            previsoes.append(y)

    if not previsoes:
        return {"erro": "Nenhum modelo encontrado."}

    previsao_final = float(np.mean(previsoes))

    return {
        "prediction": previsao_final,
        "models": modelos
    }


----------------------------------------
Arquivo: ./api/app/routers/healthcheck.py
----------------------------------------
from fastapi import APIRouter

router = APIRouter()

@router.get("/health")
def health():
    return {"status": "ok"}


----------------------------------------
Arquivo: ./api/app/routers/.gitignore
----------------------------------------
# Python
__pycache__/
*.py[cod]
*.sqlite3
*.db

# VSCode
.vscode/

# Environments
.env
.venv/
env/
venv/

# Docker
*.log
data/
.data/

# Byte-compiled / optimized / DLL files
*.so
*.o
*.pyc
*.pyo

# System files
.DS_Store
Thumbs.db

----------------------------------------
Arquivo: ./api/app/models/__init__.py
----------------------------------------
# Models init

----------------------------------------
Arquivo: ./api/app/models/models.py
----------------------------------------
# Modelos agora estÃ£o em shared/models.py

#from sqlalchemy import Column, Integer, Float, DateTime
#from app.services.database import Base
#from datetime import datetime

#class PredictionResult(Base):
#    __tablename__ = "predictions"

#    id = Column(Integer, primary_key=True, index=True)
#    #price = Column(Float)
#    #day_of_year = Column(Integer)
#    #created_at = Column(DateTime, default=datetime.utcnow)
#    symbol = Column(String)
#    prediction = Column(Float)

----------------------------------------
Arquivo: ./api/app/services/__init__.py
----------------------------------------
# Services init

----------------------------------------
Arquivo: ./api/app/services/database.py
----------------------------------------
# api/app/services/database.py
from shared.database import SessionLocal, engine, Base

----------------------------------------
Arquivo: ./api/app/main.py
----------------------------------------
##api/app/main.py

from fastapi import FastAPI
from app.routers.predict import router as predict_router

app = FastAPI(
    title="Projeto FuturoV1 API",
    version="1.0.0"
)

app.include_router(predict_router)

@app.get("/health")
def healthcheck():
    return {"status": "ok"}



----------------------------------------
Arquivo: ./api/Dockerfile
----------------------------------------
##api/Dockerfile
# Dockerfile da API

FROM python:3.11

WORKDIR /app

# instalar dependÃªncias
COPY api/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# copiar cÃ³digo
COPY api/app /app/app
COPY shared /app/shared
COPY neural_network /app/neural_network

# garantir que shared e neural_network sejam encontrÃ¡veis
ENV PYTHONPATH="/app"

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]



----------------------------------------
Arquivo: ./api/requirements.txt
----------------------------------------
fastapi
uvicorn
sqlalchemy
psycopg2-binary
pandas
numpy
joblib
python-dotenv
requests


----------------------------------------
Arquivo: ./worker/Dockerfile
----------------------------------------
##worker/Dockerfile
# Dockerfile da WORKER

FROM python:3.11

WORKDIR /worker

# Copia apenas arquivos do worker sem sobrescrever o shared
COPY worker/main.py /worker/main.py
COPY worker/requirements.txt /worker/requirements.txt
COPY worker/market_collector.py /worker/market_collector.py
COPY worker/tasks /worker/tasks

# Copia shared corretamente
COPY shared /worker/shared

# Copia a neural_network
COPY neural_network /worker/neural_network

RUN pip install --no-cache-dir -r requirements.txt

ENV PYTHONPATH="/worker"

CMD ["python", "main.py"]



----------------------------------------
Arquivo: ./worker/tasks/__init__.py
----------------------------------------
# Tasks init

----------------------------------------
Arquivo: ./worker/tasks/retrain_model.py
----------------------------------------
##worker/tasks/retrain_model,py

from nn.training.train import treinar_modelo

def executar_retreinamento():
    print("Iniciando re-treinamento do modelo...")
    treinar_modelo()
    print("Re-treinamento finalizado.")


----------------------------------------
Arquivo: ./worker/main.py
----------------------------------------
##worker/main.py

from datetime import datetime
from apscheduler.schedulers.blocking import BlockingScheduler
from shared.market_data import coletar_candles_binance
from shared.database import SessionLocal
from shared.telegram_bot import enviar_mensagem, escutar_comandos_telegram
from shared.models import PredictionResult
from shared import config
import requests
import threading
import pytz


# =======================================================
# PREVISÃƒO VIA API FASTAPI
# =======================================================
def executar_inferencia():
    try:
        response = requests.post("http://api:8000/predict", timeout=10)
        result = response.json()
        return result.get("prediction"), result.get("models")
    except Exception as e:
        enviar_mensagem(f"âŒ Erro na inferÃªncia: {e}")
        return None, {}


# =======================================================
# GRAVAR PREVISÃƒO NO POSTGRES
# =======================================================
def registrar_previsao(valor_previsto: float):
    db = SessionLocal()
    registro = PredictionResult(
        timestamp=datetime.utcnow(),
        predicted_price=valor_previsto,
        strategy_signal="",
        model_used="ensemble"
    )
    db.add(registro)
    db.commit()
    db.close()


# =======================================================
# SIMULAÃ‡ÃƒO DE OPERAÃ‡ÃƒO (ESTRATÃ‰GIA)
# =======================================================
def simular_operacao(preco_real: float, previsao: float) -> str:
    margem = 0.01  # 1%

    if previsao > preco_real * (1 + margem):
        return "COMPRA"
    elif previsao < preco_real * (1 - margem):
        return "VENDA"
    return "HOLD"


# =======================================================
# REGISTRAR OPERAÃ‡ÃƒO NO POSTGRES
# =======================================================
def registrar_operacao(tipo: str, preco_real: float, previsao: float):
    db = SessionLocal()
    resultado = ((previsao - preco_real) / preco_real) * 100

    registro = PredictionResult(
        timestamp=datetime.utcnow(),
        predicted_price=previsao,
        strategy_signal=tipo,
        model_used=f"variaÃ§Ã£o: {resultado:.2f}%"
    )
    db.add(registro)
    db.commit()
    db.close()


# =======================================================
# EXECUTAR TODA A ANÃLISE (FLUXO PRINCIPAL)
# =======================================================
def realizar_analise():
    enviar_mensagem("â± Iniciando anÃ¡lise...")

    # 1) Coletar dados da Binance
    try:
        coletar_candles_binance()
    except Exception as e:
        enviar_mensagem(f"âŒ Erro na coleta: {e}")
        return

    # 2) InferÃªncia via API
    previsao, detalhes = executar_inferencia()
    if previsao is None:
        return

    # 3) PreÃ§o real = Ãºltimo candle recebido (pegar do banco)
    from shared.market_data import carregar_candles
    df = carregar_candles(limit=1)
    preco_real = float(df["close"].iloc[-1])

    # 4) EstratÃ©gia
    tipo_operacao = simular_operacao(preco_real, previsao)

    # 5) Registrar resultados
    registrar_previsao(previsao)
    registrar_operacao(tipo_operacao, preco_real, previsao)

    # 6) Envio Telegram
    msg = (
        f"ğŸ“ˆ ExecuÃ§Ã£o concluÃ­da.\n"
        f"ğŸ“‰ Real: {preco_real:.2f}\n"
        f"ğŸ”® PrevisÃ£o: {previsao:.2f}\n\n"
        f"ğŸ§  Detalhes por modelo:\n" +
        "\n".join([f"â€¢ {k}: {v:.2f}" for k, v in detalhes.items()]) +
        f"\n\nğŸ’¡ OperaÃ§Ã£o sugerida: {tipo_operacao}"
    )
    enviar_mensagem(msg)


# =======================================================
# CALLBACK PARA COMANDOS DO TELEGRAM
# =======================================================
def executar_callback():
    realizar_analise()


# =======================================================
# INICIAR SCHEDULER E LISTENER
# =======================================================
if __name__ == "__main__":
    ativo_monitorado = "bitcoin"

    timezone = pytz.timezone("America/Sao_Paulo")
    scheduler = BlockingScheduler(timezone=timezone)
    scheduler.add_job(realizar_analise, "interval", minutes=2)

    thread = threading.Thread(
        target=escutar_comandos_telegram,
        args=(ativo_monitorado, scheduler, executar_callback)
    )
    thread.daemon = True
    thread.start()

    enviar_mensagem("ğŸ¤– Worker iniciado e aguardando rotinas...")

    scheduler.start()


----------------------------------------
Arquivo: ./worker/market_collector.py
----------------------------------------
#worker/market_colletctor.py

import requests
import pandas as pd
from shared.database import engine

def coletar_candles(symbol="BTCUSDT", interval="1m", limit=500):
    url = f"https://api.binance.com/api/v3/klines?symbol={symbol}&interval={interval}&limit={limit}"
    data = requests.get(url).json()

    rows = []
    for k in data:
        rows.append({
            "open_time": pd.to_datetime(k[0], unit="ms"),
            "open": float(k[1]),
            "high": float(k[2]),
            "low": float(k[3]),
            "close": float(k[4]),
            "volume": float(k[5]),
        })

    df = pd.DataFrame(rows)
    df.to_sql("candles", engine, if_exists="append", index=False)


----------------------------------------
Arquivo: ./worker/requirements.txt
----------------------------------------
apscheduler
requests
sqlalchemy
psycopg2-binary
pandas
numpy
python-telegram-bot==13.15
python-dotenv
joblib
pyyaml


----------------------------------------
Arquivo: ./config/config.yaml
----------------------------------------
app_name: ProjetoFuturo

----------------------------------------
Arquivo: ./scripts/coleta/coleta_cripto.py
----------------------------------------
# scripts/coleta/coleta_cripto.py
import requests
import pandas as pd
from datetime import datetime
import os
import argparse

def coletar_dados(ativo='bitcoin', vs_currency='usd', dias=30, intervalo='daily', output_path='data/input'):
    url = f'https://api.coingecko.com/api/v3/coins/{ativo}/market_chart'
    params = {
        'vs_currency': vs_currency,
        'days': dias,
        'interval': intervalo
    }

    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
        data = r.json()

        prices = pd.DataFrame(data['prices'], columns=["timestamp", "price"])
        prices['timestamp'] = pd.to_datetime(prices['timestamp'], unit='ms')

        os.makedirs(output_path, exist_ok=True)
        caminho_saida = os.path.join(output_path, f"{ativo}_{intervalo}_{dias}d.csv")
        prices.to_csv(caminho_saida, index=False)
        print(f"âœ… Dados salvos em: {caminho_saida}")

    except requests.exceptions.RequestException as e:
        print(f"âŒ Erro ao coletar dados de {ativo}: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Coleta de dados de criptomoedas via CoinGecko")
    parser.add_argument('--ativo', type=str, default='bitcoin', help='Nome do ativo (ex: bitcoin, ethereum)')
    parser.add_argument('--dias', type=int, default=30, help='Quantos dias de histÃ³rico')
    parser.add_argument('--intervalo', type=str, default='daily', choices=['minutely', 'hourly', 'daily'], help='Intervalo de dados')
    parser.add_argument('--saida', type=str, default='data/input', help='DiretÃ³rio de saÃ­da')

    args = parser.parse_args()

    coletar_dados(
        ativo=args.ativo,
        dias=args.dias,
        intervalo=args.intervalo,
        output_path=args.saida
    )


----------------------------------------
Arquivo: ./scripts/bin/iniciar.sh
----------------------------------------
#!/bin/bash
cd /home/aplicacao/projetos/projeto_futuroV1
docker compose up --build -d


----------------------------------------
Arquivo: ./scripts/bin/start_all.sh
----------------------------------------
#!/bin/bash
echo 'Start all containers...'

----------------------------------------
Arquivo: ./README.md
----------------------------------------
# Projeto_FuturoV1

O **Projeto_FuturoV1** Ã© uma aplicaÃ§Ã£o modular baseada em contÃªineres Docker, composta por trÃªs serviÃ§os principais:

- **API (FastAPI)**: expÃµe uma interface HTTP para inferÃªncia de dados.
- **Worker**: agenda tarefas recorrentes para execuÃ§Ã£o e grava dados em um banco local.
- **NN (Neural Network)**: executa inferÃªncia com base em um modelo predefinido.

## Arquitetura

Projeto_FuturoV1/
â”œâ”€â”€ api/ â†’ API REST com FastAPI
â”œâ”€â”€ worker/ â†’ ServiÃ§o de agendamento e persistÃªncia de previsÃµes
â”œâ”€â”€ nn/ â†’ ServiÃ§o de rede neural
â”œâ”€â”€ .data/ â†’ Volume persistente compartilhado (banco SQLite)
â”œâ”€â”€ docker-compose.yml â†’ Orquestrador de containers
â”œâ”€â”€ .gitignore â†’ Ignora arquivos que nÃ£o devem ser versionados
â””â”€â”€ README.md â†’ Este arquivo


---

## ServiÃ§os

### ğŸ”¹ API (FastAPI)
- **Local**: `api/`
- **Imagem**: `projeto_futurov1-api`
- **Porta exposta**: `8000`
- **Endpoints principais**:
  - `GET /healthcheck` â†’ Verifica se a API estÃ¡ ativa
  - `POST /predictions` â†’ Recebe requisiÃ§Ãµes e consulta a NN

### ğŸ”¹ Worker (Agendador)
- **Local**: `worker/`
- **Imagem**: `projeto_futurov1-worker`
- **FunÃ§Ã£o**: Roda a cada intervalo (via cron ou APScheduler) e grava previsÃµes em `/data/app.db`

### ğŸ”¹ NN (Rede Neural)
- **Local**: `nn/`
- **Imagem**: `projeto_futurov1-nn`
- **FunÃ§Ã£o**: Recebe chamadas da API e retorna previsÃµes simuladas ou reais.

---

## Como executar

### âœ… PrÃ©-requisitos

- Docker
- Docker Compose
- Git

### ğŸ”§ ConfiguraÃ§Ã£o

Clone o repositÃ³rio:

```bash
git clone git@github.com:SEU_USUARIO/projetoFuturoV1.git
cd projetoFuturoV1

Construa e suba os serviÃ§os:
docker compose up --build -d

Verifique se a API estÃ¡ no ar:
curl http://localhost:8000/healthcheck

ManutenÃ§Ã£o
ğŸ“„ Ver logs dos serviÃ§os

docker compose logs api
docker compose logs worker
docker compose logs nn

ğŸš« Parar e remover containers

docker compose down

ğŸ”„ Reconstruir todos os serviÃ§os
docker compose up --build -d

Volume de Dados

Os serviÃ§os compartilham um volume local .data/ onde o banco SQLite app.db Ã© armazenado. Este volume Ã© montado em /data dentro dos containers.
Git & Versionamento

Antes de subir alteraÃ§Ãµes:

git add .
git commit -m "Mensagem clara do que foi alterado"
git push origin main

Certifique-se de que o .gitignore ignora .data/, __pycache__/, arquivos .pyc, etc.

Endpoints principais

### ğŸ”¹ API (FastAPI)
- **Local**: `api/`
- **Imagem**: `projeto_futurov1-api`
- **Porta exposta**: `8000`

#### Endpoints principais:

| MÃ©todo | Rota               | DescriÃ§Ã£o                                      |
|--------|--------------------|-----------------------------------------------|
| GET    | `/healthcheck`     | Verifica se a API estÃ¡ no ar                  |
| POST   | `/predictions`     | Recebe parÃ¢metros, aciona o serviÃ§o NN e retorna a previsÃ£o |


SeguranÃ§a

    O SQLite estÃ¡ isolado via volume Docker.

    Nenhum dado sensÃ­vel Ã© armazenado ou versionado no Git.

    Para produÃ§Ã£o, considere usar PostgreSQL e autenticaÃ§Ã£o JWT na API.

ContribuiÃ§Ã£o

Sinta-se livre para abrir issues, sugerir melhorias ou criar Pull Requests!
LicenÃ§a

Este projeto estÃ¡ licenciado sob a MIT License.


----------------------------------------
Arquivo: ./shared/__init__.py
----------------------------------------
# Shared module init

----------------------------------------
Arquivo: ./shared/telegram_bot.py
----------------------------------------
import os
import time
import requests
from dotenv import load_dotenv

load_dotenv()
BOT_TOKEN = os.getenv("BOT_TOKEN")
CHAT_ID = os.getenv("CHAT_ID")

if not BOT_TOKEN or not CHAT_ID:
    raise ValueError("BOT_TOKEN ou CHAT_ID nÃ£o definidos no arquivo .env")

URL_BASE = f"https://api.telegram.org/bot{BOT_TOKEN}"

# Enviar mensagem simples
def enviar_mensagem(mensagem: str):
    try:
        url = f"{URL_BASE}/sendMessage"
        payload = {
            "chat_id": CHAT_ID,
            "text": mensagem
        }
        response = requests.post(url, json=payload, timeout=10)
        response.raise_for_status()
    except Exception as e:
        print(f"âŒ Erro ao enviar mensagem: {e}")

# Escutar comandos via long polling
def escutar_comandos_telegram(ativo_monitorado, scheduler, executar_callback):
    print("ğŸ” Escutando comandos do Telegram...")
    offset = None

    while True:
        try:
            params = {"timeout": 10, "offset": offset}
            resp = requests.get(f"{URL_BASE}/getUpdates", params=params, timeout=15)
            resp.raise_for_status()
            updates = resp.json()

            for update in updates.get("result", []):
                offset = update["update_id"] + 1
                mensagem = update.get("message", {})
                chat_id = str(mensagem.get("chat", {}).get("id", ""))
                texto = mensagem.get("text", "").strip()

                # Ignorar mensagens de outros usuÃ¡rios
                if chat_id != CHAT_ID:
                    continue

                if texto.lower() == "/status":
                    enviar_mensagem(f"ğŸ¤– Bot ativo. Monitorando {ativo_monitorado['nome']}.")
                elif texto.startswith("/alterar"):
                    partes = texto.split()
                    if len(partes) == 2:
                        ativo_monitorado["nome"] = partes[1]
                        enviar_mensagem(f"ğŸ”„ Ativo alterado para: {ativo_monitorado['nome']}")
                    else:
                        enviar_mensagem("âŒ Formato invÃ¡lido. Use: /alterar bitcoin")
                elif texto.lower() == "/executar":
                    enviar_mensagem("â± Executando anÃ¡lise manual agora...")
                    executar_callback()  # Chamada direta para funÃ§Ã£o de execuÃ§Ã£o
                else:
                    enviar_mensagem("ğŸ¤– Comando nÃ£o reconhecido. Use /status ou /alterar <ativo>.")

        except Exception as e:
            print(f"âŒ Erro no polling do Telegram: {e}")
            time.sleep(5)  # Espera e tenta de novo


----------------------------------------
Arquivo: ./shared/config.py
----------------------------------------
## shared/config.py

import os

# ---------------------------------------------
# ParÃ¢metros gerais
# ---------------------------------------------
ATIVO = os.getenv("ATIVO", "BTCUSDT")
INTERVALO = os.getenv("INTERVALO", "1m")

# Quantidade de candles utilizados pela IA
NUM_CANDLES_USADOS = int(os.getenv("NUM_CANDLES_USADOS", 60))

# Quantidade de candles para coleta
QUANTIDADE_CANDLES = int(os.getenv("QUANTIDADE_CANDLES", 300))


# ---------------------------------------------
# Caminhos e modelos
# ---------------------------------------------
CAMINHO_DADOS = f"/data/input/{ATIVO}_{INTERVALO}.csv"
DIRETORIO_MODELO = "/data/modelo"

NOME_MODELO = os.getenv("NOME_MODELO", "LinearRegression")
CAMINHO_MODELO = f"{DIRETORIO_MODELO}/modelo_{NOME_MODELO}.pkl"



----------------------------------------
Arquivo: ./shared/database.py
----------------------------------------
# shared/database.py

import os
import pandas as pd
from sqlalchemy import create_engine, text, Column, Integer, Float, String, DateTime
from sqlalchemy.orm import sessionmaker, declarative_base

# ------------------------------
# ConfiguraÃ§Ãµes do Banco
# ------------------------------
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASS = os.getenv("DB_PASS", "postgres")
DB_NAME = os.getenv("DB_NAME", "futuro")
DB_HOST = os.getenv("DB_HOST", "postgres")  # O nome do serviÃ§o no docker-compose
DB_PORT = os.getenv("DB_PORT", "5432")

DATABASE_URL = f"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()


# ------------------------------
# Modelo da tabela CANDLES
# ------------------------------
class Candle(Base):
    __tablename__ = "candles"

    id = Column(Integer, primary_key=True, index=True)
    timestamp = Column(DateTime, index=True)
    open = Column(Float)
    high = Column(Float)
    low = Column(Float)
    close = Column(Float)
    volume = Column(Float)


# ------------------------------
# Criar tabelas se nÃ£o existirem
# ------------------------------
def init_db():
    Base.metadata.create_all(bind=engine)


# ------------------------------
# FunÃ§Ã£o usada pela API /predict
# ------------------------------
def carregar_candles(limit=100):
    """Carrega candles do PostgreSQL e retorna um DataFrame."""
    with SessionLocal() as session:
        sql = text("""
            SELECT timestamp, open, high, low, close, volume
            FROM candles
            ORDER BY timestamp DESC
            LIMIT :limit
        """)

        rows = session.execute(sql, {"limit": limit}).fetchall()

        if not rows:
            return None

        df = pd.DataFrame(rows, columns=["timestamp", "open", "high", "low", "close", "volume"])
        df.sort_values("timestamp", inplace=True)
        return df



----------------------------------------
Arquivo: ./shared/models.py
----------------------------------------
# shared/models.py

from sqlalchemy import Column, Integer, Float, String, DateTime
from datetime import datetime
from shared.database import Base


class Candle(Base):
    __tablename__ = "candles"

    id = Column(Integer, primary_key=True)
    open_time = Column(DateTime, index=True)
    open = Column(Float)
    high = Column(Float)
    low = Column(Float)
    close = Column(Float)
    volume = Column(Float)


class PredictionResult(Base):
    __tablename__ = "prediction_results"

    id = Column(Integer, primary_key=True)
    timestamp = Column(DateTime, default=datetime.utcnow)
    predicted_price = Column(Float)
    strategy_signal = Column(String)
    model_used = Column(String)




----------------------------------------
Arquivo: ./shared/market_data.py
----------------------------------------
# shared/market_data.py

import requests
import pandas as pd
from sqlalchemy import text
from shared.database import engine, SessionLocal
from shared.models import Candle
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


# ---------------------------------------------------------
# SALVAR CANDLES NO POSTGRES
# ---------------------------------------------------------
def salvar_candles(df: pd.DataFrame):
    try:
        df.to_sql("candles", engine, if_exists="append", index=False)
        logger.info(f"{len(df)} candles inseridos no banco.")
    except Exception as e:
        logger.error(f"Erro ao salvar candles: {e}")


# ---------------------------------------------------------
# CARREGAR CANDLES DO POSTGRES
# ---------------------------------------------------------
def carregar_candles(limit=500):
    query = text("""
        SELECT open_time, open, high, low, close, volume
        FROM candles
        ORDER BY open_time DESC
        LIMIT :limit
    """)

    df = pd.read_sql(query, engine, params={"limit": limit})
    df.sort_values("open_time", inplace=True)
    return df


# ---------------------------------------------------------
# COLETAR CANDLES DA BINANCE E SALVAR NO BANCO
# ---------------------------------------------------------
def coletar_candles_binance(symbol="BTCUSDT", interval="1m", limit=500):
    url = (
        "https://api.binance.com/api/v3/klines"
        f"?symbol={symbol}&interval={interval}&limit={limit}"
    )

    try:
        data = requests.get(url, timeout=10).json()
    except Exception as e:
        logger.error(f"Erro ao coletar dados da Binance: {e}")
        return None

    candles = []
    for k in data:
        candles.append(
            {
                "open_time": datetime.fromtimestamp(k[0] / 1000),
                "open": float(k[1]),
                "high": float(k[2]),
                "low": float(k[3]),
                "close": float(k[4]),
                "volume": float(k[5]),
            }
        )

    df = pd.DataFrame(candles)

    # salvar no banco
    salvar_candles(df)

    return df


----------------------------------------
Arquivo: ./data/trades.db
----------------------------------------
[BINÃRIO OU NÃƒO-TEXTO - conteÃºdo nÃ£o incluÃ­do]


----------------------------------------
Arquivo: ./data/app.db
----------------------------------------
[BINÃRIO OU NÃƒO-TEXTO - conteÃºdo nÃ£o incluÃ­do]


----------------------------------------
Arquivo: ./.gitignore
----------------------------------------

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# Caches
*.cache
*.log

# Virtual environments
.env/
.venv/

# SQLite DB (opcional)
*.sqlite3
*.db

# Pytest
.pytest_cache/

# Jupyter Notebook checkpoints
.ipynb_checkpoints

# VS Code / IDEs
.vscode/
.idea/

# Docker
*.pid
*.sock
docker-compose.override.yml

# Data volumes
.data/

# Envs ou secrets
.env
.env.*

# System
.DS_Store
Thumbs.db

# Git
*.swp


----------------------------------------
Arquivo: ./init/db_init.py
----------------------------------------
# init/db_init.py

from shared.database import init_db
from shared.database import engine
import time

def inicializar_banco():
    print("Aguardando o PostgreSQL ficar pronto...")

    # aguarda o postgres estar acessÃ­vel
    for i in range(10):
        try:
            with engine.connect() as conn:
                print("PostgreSQL conectado!")
                break
        except:
            print("Tentativa", i+1, "/ 10 - PostgreSQL ainda nÃ£o estÃ¡ pronto...")
            time.sleep(3)

    print("Criando tabelas...")
    init_db()
    print("Tabelas criadas com sucesso!")


if __name__ == "__main__":
    inicializar_banco()





----------------------------------------
Arquivo: ./init/Dockerfile
----------------------------------------
# init/Dockerfile

FROM python:3.11

WORKDIR /init

COPY init /init
COPY shared /init/shared
COPY neural_network /init/neural_network

COPY init/requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt

ENV PYTHONPATH="/init"

CMD ["python", "db_init.py"]


----------------------------------------
Arquivo: ./init/requirements.txt
----------------------------------------

sqlalchemy
psycopg2-binary
python-dotenv


----------------------------------------
Arquivo: ./test_telegram.py
----------------------------------------
# test_telegram.py
import os
import requests
from dotenv import load_dotenv

load_dotenv()

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHAT_ID = os.getenv("CHAT_ID")

message = "ğŸš€ Teste de envio via Telegram bot."

url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
response = requests.post(url, data={"chat_id": CHAT_ID, "text": message})

print("Status:", response.status_code)
print(response.text)



----------------------------------------
Arquivo: ./docker-compose.yml
----------------------------------------
services:
  db:
    image: postgres:15
    container_name: postgres
    restart: always
    env_file: .env
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASS}
      POSTGRES_DB: ${DB_NAME}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - backend
    ports:
      - "5432:5432"

  db_init:
    container_name: futurov1_db_init
    build:
      context: .
      dockerfile: init/Dockerfile
    depends_on:
      - db
    env_file: .env
    environment:
      DATABASE_URL: ${DATABASE_URL}
    networks:
      - backend

  api:
    container_name: futurov1_api
    build:
      context: .
      dockerfile: api/Dockerfile
    env_file: .env
    environment:
      DATABASE_URL: ${DATABASE_URL}
    volumes:
      - ./shared:/shared
      - ./neural_network:/neural_network  
    depends_on:
      - db
      - db_init
    networks:
      - backend
    ports:
      - "8000:8000"

  worker:
    container_name: futurov1_worker
    build:
      context: .
      dockerfile: worker/Dockerfile
    env_file: .env
    environment:
      DATABASE_URL: ${DATABASE_URL}
      TELEGRAM_TOKEN: ${TELEGRAM_TOKEN}
      TELEGRAM_CHAT_ID: ${TELEGRAM_CHAT_ID}
    volumes:
      - ./shared:/shared
      - ./neural_network:/neural_network  
    depends_on:
      - api
      - db
    networks:
      - backend

  grafana:
    image: grafana/grafana
    container_name: futurov1_grafana
    env_file: .env
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASS}
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - db
    networks:
      - backend

  pgadmin:
    image: dpage/pgadmin4
    container_name: futurov1_pgadmin
    env_file: .env
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASS}
    ports:
      - "8080:80"
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    networks:
      - backend
    depends_on:
      - db

  heimdall:
    image: linuxserver/heimdall
    container_name: futurov1_heimdall
    env_file: .env
    environment:
      PUID: 1000
      PGID: 1000
      TZ: America/Sao_Paulo
    ports:
      - "8090:80"
    volumes:
      - heimdall_data:/config
    networks:
      - backend

networks:
  backend:

volumes:
  postgres_data:
  grafana_data:
  pgadmin_data:
  heimdall_data:


----------------------------------------
Arquivo: ./requirements.txt
----------------------------------------

##/opt/Projetos/Projeto_FuturoV1/requirements.txt

anyio==4.9.0
certifi==2025.4.26
charset-normalizer==3.4.2
coverage==7.8.1
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.10
iniconfig==2.1.0
packaging==25.0
pluggy==1.6.0
pytest==8.3.5
pytest-cov==6.1.1
python-dotenv==1.1.0
requests==2.32.3
requests-mock==1.12.1
sniffio==1.3.1
urllib3==2.4.0
fastapi==0.111.0
uvicorn[standard]==0.30.1
pandas==2.2.2


----------------------------------------
Arquivo: ./.env
----------------------------------------
#projetofuturo/.env
#No futuro mudar para o vault

BOT_TOKEN=7199722766:AAHaVdaV4VbkxYzBBV8EFMd0a-MuqYj9xdE
CHAT_ID=717731723

#POSTGRES_USER=trader
#POSTGRES_PASSWORD=trader123
#POSTGRES_DB=projetofuturo
#DB_HOST=postgres
#POSTGRES_PORT=5432

DB_USER=futuro
DB_PASS=senha123
DB_NAME=futurov1
DATABASE_URL=postgresql+psycopg2://futuro:senha123@db:5432/futurov1

TELEGRAM_TOKEN=seu_token
TELEGRAM_CHAT_ID=123456789

GRAFANA_PASS=admin123
PGADMIN_EMAIL=admin@local.com
PGADMIN_PASS=admin123

----------------------------------------
Arquivo: ./neural_network/training/train.py
----------------------------------------
##nn/training/train.py

from .train_linear import treinar_modelo_linear
from .train_rf import treinar_modelo_rf
from .train_xgb import treinar_modelo_xgb

def treinar_todos_modelos():
    treinar_modelo_linear()
    treinar_modelo_rf()
    treinar_modelo_xgb()




----------------------------------------
Arquivo: ./neural_network/training/__init__.py
----------------------------------------
# Training init

----------------------------------------
Arquivo: ./neural_network/training/treinar_modelo.py
----------------------------------------
# nn/training/treinar_modelo.py
import pandas as pd
from sklearn.linear_model import LinearRegression
import pickle

df = pd.read_csv('dados/btc.csv')
df['timestamp'] = pd.to_datetime(df['timestamp'])
df['dia'] = df['timestamp'].dt.dayofyear
X = df[['dia']]
y = df['price']

modelo = LinearRegression()
modelo.fit(X, y)

with open('nn/modelos/modelo_btc.pkl', 'wb') as f:
    pickle.dump(modelo, f)

print("Modelo treinado e salvo em nn/modelos/modelo_btc.pkl")

# Enviar notificaÃ§Ã£o ao finalizar
from shared.telegram_bot import notificar_telegram
notificar_telegram("Re-treinamento finalizado com sucesso!")

----------------------------------------
Arquivo: ./neural_network/training/train_bkp2.py
----------------------------------------
##neural_network/training/train.py

import pandas as pd
from sklearn.linear_model import LinearRegression
import joblib
import os

def treinar_modelo():
    print("ğŸ“š Iniciando treinamento do modelo...")

    # Caminho para o arquivo CSV
    csv_path = "/data/input/bitcoin_daily_30d.csv"
    df = pd.read_csv(csv_path)

    # Garante que os dados necessÃ¡rios estÃ£o presentes
    features = ["open", "high", "low", "volume"]
    target = "close"  # PrevisÃ£o do preÃ§o de fechamento

    for coluna in features + [target]:
        if coluna not in df.columns:
            raise ValueError(f"Coluna obrigatÃ³ria ausente: {coluna}")

    # Remove possÃ­veis NaNs
    df.dropna(subset=features + [target], inplace=True)

    # Separa X e y
    X = df[features]
    y = df[target]

    # Treina o modelo
    modelo = LinearRegression()
    modelo.fit(X, y)

    # Salva o modelo
    os.makedirs("/data/modelo", exist_ok=True)
    joblib.dump(modelo, "/data/modelo/modelo.pkl")
    print("âœ… Modelo treinado e salvo em /data/modelo/modelo.pkl")


----------------------------------------
Arquivo: ./neural_network/training/train_bkp3.py
----------------------------------------
##neural_network/training/train.py

import pandas as pd
from sklearn.linear_model import LinearRegression
import joblib
import os
from datetime import datetime
from shared import config

def treinar_modelo():
    print("ğŸ¤– Iniciando treinamento do modelo...")

    # Carrega os dados
    df = pd.read_csv(config.CAMINHO_DADOS)

    # Gera as novas features
    df["dia"] = pd.to_datetime(df["open_time"]).dt.dayofyear
    df["media"] = (df["high"] + df["low"]) / 2
    df["var_percent"] = ((df["high"] - df["low"]) / df["open"]) * 100

    # Define as features e variÃ¡vel alvo
    X = df[["open", "high", "low", "volume", "dia", "media", "var_percent"]]
    y = df["close"]

    # Treinamento
    modelo = LinearRegression()
    modelo.fit(X, y)

    # Gera nome com timestamp
    timestamp = datetime.utcnow().strftime("%Y%m%d%H%M%S")
    nome_arquivo = f"modelo_{timestamp}.pkl"
    caminho_modelo = os.path.join(config.DIRETORIO_MODELO, nome_arquivo)

    # Salva o modelo
    os.makedirs(config.DIRETORIO_MODELO, exist_ok=True)
    joblib.dump(modelo, caminho_modelo)
    print(f"âœ… Modelo salvo em: {caminho_modelo}")

    # Cria/atualiza o link simbÃ³lico
    link_simbolico = os.path.join(config.DIRETORIO_MODELO, "modelo_mais_recente.pkl")
    if os.path.islink(link_simbolico) or os.path.exists(link_simbolico):
        os.remove(link_simbolico)
    os.symlink(nome_arquivo, link_simbolico)
    print(f"ğŸ”— Link simbÃ³lico criado: {link_simbolico} -> {nome_arquivo}")



----------------------------------------
Arquivo: ./neural_network/training/train_linear.py
----------------------------------------
##nn/training/train_linear.py

from sklearn.linear_model import LinearRegression
import joblib, os
from datetime import datetime
from shared import config
from .utils import preparar_dados

def treinar_modelo_linear():
    X, y = preparar_dados()
    modelo = LinearRegression()
    modelo.fit(X, y)

    caminho = os.path.join(config.DIRETORIO_MODELOS, f"linear_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.pkl")
    joblib.dump(modelo, caminho)
    print(f"âœ… Modelo Linear salvo em: {caminho}")


----------------------------------------
Arquivo: ./neural_network/training/train_rf.py
----------------------------------------
##nn/training/train_rf.py

from sklearn.ensemble import RandomForestRegressor
import joblib, os
from datetime import datetime
from shared import config
from .utils import preparar_dados

def treinar_modelo_rf():
    X, y = preparar_dados()
    modelo = RandomForestRegressor(n_estimators=100, random_state=42)
    modelo.fit(X, y)

    caminho = os.path.join(config.DIRETORIO_MODELOS, f"rf_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.pkl")
    joblib.dump(modelo, caminho)
    print(f"âœ… Modelo RF salvo em: {caminho}")


----------------------------------------
Arquivo: ./neural_network/training/train_xgb.py
----------------------------------------
from xgboost import XGBRegressor
import joblib, os
from datetime import datetime
from shared import config
from .utils import preparar_dados

def treinar_modelo_xgb():
    X, y = preparar_dados()
    modelo = XGBRegressor(n_estimators=100, random_state=42)
    modelo.fit(X, y)

    caminho = os.path.join(config.DIRETORIO_MODELOS, f"xgb_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.pkl")
    joblib.dump(modelo, caminho)
    print(f"âœ… Modelo XGBoost salvo em: {caminho}")


----------------------------------------
Arquivo: ./neural_network/training/utils.py
----------------------------------------
##nn/training/utils.py

import pandas as pd
from shared import config

def preparar_dados():
    df = pd.read_csv(config.CAMINHO_DADOS)

    df["dia"] = pd.to_datetime(df["open_time"]).dt.dayofyear
    df["media"] = (df["high"] + df["low"]) / 2
    df["var_percent"] = ((df["high"] - df["low"]) / df["open"]) * 100
    df["target"] = df["close"].shift(-config.ANTECEDENCIA_CANDLES)
    df.dropna(inplace=True)

    X = df[["open", "high", "low", "volume", "dia", "media", "var_percent"]]
    y = df["target"]
    return X, y




----------------------------------------
Arquivo: ./neural_network/training/treinar_linear.py
----------------------------------------
##neural_network/training/train_linear.py

import pandas as pd
import joblib
from sklearn.linear_model import LinearRegression
from shared.market_data import carregar_candles
import os

MODELOS_DIR = "/data/modelos"
os.makedirs(MODELOS_DIR, exist_ok=True)

def treinar_linear():
    df = carregar_candles(limit=5000)

    df["return"] = df["close"].pct_change()
    df.dropna(inplace=True)

    X = df[["open", "high", "low", "close", "volume", "return"]]
    y = df["close"].shift(-1).dropna()
    X = X.iloc[:-1]

    modelo = LinearRegression()
    modelo.fit(X, y)

    caminho = os.path.join(MODELOS_DIR, "linear_regression.pkl")
    joblib.dump(modelo, caminho)

    print(f"Modelo salvo em: {caminho}")


if __name__ == "__main__":
    treinar_linear()


----------------------------------------
Arquivo: ./neural_network/models/__init__.py
----------------------------------------
# Models NN init

----------------------------------------
Arquivo: ./neural_network/models/models.py
----------------------------------------
from sqlalchemy import Column, Integer, String, Float
from app.services.database import Base

class TradeModel(Base):
    __tablename__ = "trades"

    id = Column(Integer, primary_key=True, index=True)
    symbol = Column(String)
    prediction = Column(Float)


----------------------------------------
Arquivo: ./neural_network/inference/predict.py
----------------------------------------
# nn/inference/predict.py

import numpy as np
import joblib
import os
import glob
import pandas as pd
from shared.market_data import carregar_candles

# Caminho padrÃ£o para modelos
MODELOS_DIR = "/data/modelos"


# ----------------------------------------------------
# CARREGAR MODELOS
# ----------------------------------------------------
def carregar_modelos():
    modelos = {}
    arquivos = glob.glob(os.path.join(MODELOS_DIR, "*.pkl"))

    for caminho in arquivos:
        nome = os.path.basename(caminho).replace(".pkl", "")
        modelos[nome] = joblib.load(caminho)

    return modelos


# ----------------------------------------------------
# PREVER PREÃ‡O
# ----------------------------------------------------
def prever():
    # LER DADOS DO POSTGRES
    df = carregar_candles(limit=200)

    # CRIA FEATURES SIMPLES (vocÃª pode expandir isso depois)
    df["return"] = df["close"].pct_change()
    df.dropna(inplace=True)

    X = df[["open", "high", "low", "close", "volume", "return"]].values[-1].reshape(1, -1)

    modelos = carregar_modelos()
    previsoes = {}

    for nome, modelo in modelos.items():
        previsoes[nome] = float(modelo.predict(X)[0])

    media_previsoes = np.mean(list(previsoes.values()))

    return float(media_previsoes), previsoes


----------------------------------------
Arquivo: ./neural_network/requirements_neural_network.txt
----------------------------------------
scikit-learn==1.5.0
xgboost==2.0.3
pandas==2.2.2
joblib==1.4.2

